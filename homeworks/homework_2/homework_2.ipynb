{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction: The purpose of this homework will be to identify potential jargon. In order to accomplish this we will employ two small small document corpora. One will be related to news and the other will be from government documents. Your task will be to implement a simple jargon identifier.\n",
    "\n",
    "In order to accomplish this task you will need to perform several steps. The first relates to identifying the word distributions in the base corpus. We will be using the Reuters news corpus as it is freely available in NLTK. For a jargon corpus we will be making use of DOD OIG reports that we have seen in the past. \n",
    "\n",
    "A first step will be to tokenize the documents in both sets. Then, you will try to simply look at the differences in tokens seen. \n",
    "\n",
    "But just looking at token differences isn't very sophisticated. We w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Get a sample corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "# if this fails you might need to ensure that you've downloaded this resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_corpus_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(reuters.fileids())\n",
    "#  this has a large number of files... \n",
    "# you might wish to limit the number of documents you use while developing your technique \n",
    "# ex. reuters.fileids()[0:25]\n",
    "for doc in reuters.fileids(): \n",
    "    doc_text = reuters.open(doc).read()\n",
    "    # this doc_text variable will give you a text version of the news article. This could be tokenized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Get a potential jargon corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.util import bigrams \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "dir_base = \"/repos/GWU/gwu_nlp_f19/homeworks/homework_2/data/\"\n",
    "\n",
    "\n",
    "####\n",
    "# Notice: We are reusing code from class notes... remember these kind of building blocks\n",
    "####\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename , encoding='utf-8').read()\n",
    "    return input_file_text\n",
    "\n",
    "    \n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    for f in files:\n",
    "        file_text = read_file(join(directory, f))\n",
    "        file_texts.append({\"file\":f, \"content\": file_text })\n",
    "    return file_texts\n",
    "    \n",
    "jargon_corpus = read_directory_files(dir_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perhaps you could store tokens from the jargon corpus here\n",
    "jargon_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in jargon_corpus:\n",
    "    # Here you might try to identify relevant tokens from the jargon corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Identify words unique to the jargon corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have two sets of tokens, you could attempt to simply \n",
    "# identify what the difference of membership between the two sets might be.\n",
    "real_jargon = set(jargon_tokens) # how might you compare this to your list of tokens above to identify distinct elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you might want to look at the real jargon list and see if it looks right to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Refine differences in corpora using frequency information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will get the token frequency distribution from the corpora\n",
    "freq_jargon = nltk.FreqDist(jargon_tokens)\n",
    "freq_base_tokens = nltk.FreqDist(base_corpus_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to see what is going on\n",
    "freq_base_tokens.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_jargon.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, what are the most common tokens from the jargon corpus?\n",
    "freq_jargon.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might also wish to calculate a probability distribution for the tokens. \n",
    "# you could use either the maximum likelihood estimator or the smoothed distribution\n",
    "# so you need to set those distributions here (we discussed in class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once you have these probability distributions you will want to look in the jargon corpus and \n",
    "# find tokens that have a higher probability of occurence than in the base corpus\n",
    "\n",
    "high_probability_jargon = []\n",
    "\n",
    "for token in jargon:\n",
    "    # get its probability in the jargon corpus\n",
    "    # get its probability base corpus\n",
    "    # if the jargon corpus probability for the token is higher, then record the item as jargon\n",
    "    # also, you could use a significance test with the probaility of jargon to see if they are \n",
    "    # from different distributions b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this cell, please put a short writeup of your approach and comments on your results. The goal here is to explain how well you think your method worked based on looking at some of your output data. Additionally, please describe things you might do fifferently or ways in which you might improve the process if you were given more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Modify your code to handle bigrams (it's pretty simple if you think about it)\n",
    "2. Consider how you might filter out words that are jargon specific, but are reference to real items. That is, filter out words that include some combination of numbers and characters\n",
    "3. Try to build a method to determine the probability of sentences coming from either the jargon corpus or the base corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
